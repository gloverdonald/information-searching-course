Установка
и
настройка
KVM
под
управлением
CentOS
/
Хабр
β
фев
в
:
Установка
и
настройка
KVM
под
управлением
CentOS
мин
K
Туториал
Приветствую
вас
Хабражители!
Сегодня
хочу
поделиться
с
вами
одним
из
своих
наработанных
мануалов
который
отточен
многоразовым
применением
про
который
с
уверенностью
могу
сказать
что
«точно
работает!»
без
лишних
танцев
с
бубном
Ориентирована
статья
скорее
на
начинающих
системных
администраторов
чем
на
гуру
(для
них
тут
ничего
нового
нет
:)
)
и
в
ней
я
постараюсь
раскрыть
рабочий
и
довольно
быстрый
вариант
развертывания
сервера
виртуальных
машин
стараясь
при
этом
охватись
как
можно
больше
нюансов
и
подводных
камней
Однако
буду
рад
вниманию
знающих
и
опытных
админов
которые
возможно
дадут
дельные
советы
и
помогут
поправить
ошибки
если
таковые
имеются
Disclaimer
Поправьте
если
не
так
но
в
поиске
я
не
нашел
реализации
данной
задачи
именно
на
CentOS
с
подробным
описанием
всех
шагов
для
новичков
Хорошая
серия
статей
написана
но
они
для
Debian
Естественно
для
бывалых
админов
в
этом
никакой
проблемы
нет
но
повторюсь
моя
задача
—
описать
подробную
инструкцию
для
новичков
Вопрос:
в
Интернете
есть
множество
руководств
для
установки
Qemu
KVM
под
CentOS
возразите
вы
и
чем
же
данная
статья
будет
интересна?
Ответ:
здесь
описывается
полный
цикл
установки
и
настройки
необходимых
для
виртуализации
компонентов
установка
гостевых
виртуальных
машин
(ВМ)
настройка
белой
и
серой
сети
для
ВМ
а
также
некоторые
аспекты
которые
помогут
упростить
управление
ВМ
используя
проброс
графики
с
удаленного
сервера
на
свой
ПК
и
запуском
virt-manager
Помните
Начнем
с
того
что
если
Вы
читаете
это
то
у
вас
уже
готова
ОС
CentOS
(я
использовал
версию
)
причем
для
установки
гостевых
ВМ
разной
битности
(
или
)
хост-сервер
(физический
сервер
на
котором
и
будем
устанавливать
KVM
вместе
с
ВМ)
должен
быть
именно
с
-битной
ОС
Все
действия
выполняются
из-под
пользователя
root
Итак
приступим
к
руководству
Шаг
—
Подготовка
Проверяем
поддерживает
ли
CPU
аппаратную
виртуализацию:
#
egrep
'(vmx|svm)'
/proc/cpuinfo

Если
вывод
не
пустой
значит
—
процессор
поддерживает
аппаратную
виртуализацию
Кому
интересно
все
действия
выполнялись
на
конфигурации
Intel
Xeon
Quad
Core
E
-
GHz
/
GB
/
x
TB
Устанавливаем
KVM
и
библиотеки
виртуализации:
#
yum
install
kvm
libvirt

Запускаем
сервис
KVM
#
service
libvirtd
start

Смотрим
загружен
ли
модуль
KVM
#
lsmod
|
grep
kvm

Должны
получить
вывод:
kvm_intel

kvm
kvm_intel

В
данном
случае
видим
что
загружен
модуль
kvm_intel
так
как
произволитель
CPU
—
Intel
Проверка
подключения
к
KVM
#
virsh
sysinfo

Должны
получить
вывод:
<sysinfo
type='smbios'>

<bios>

<entry
name='vendor'>HP</entry>

<entry
name='version'>J
</entry>



Шаг
—
Создание
хранилища
для
виртуальных
машин
(Storage
Pool)
приводится
описание
как
настроить
хранилище
разных
видов
В
рассматриваемом
же
примере
описан
простой
тип
хранилища
—
для
каждой
ВМ
создается
свой
файл
*
img
под
виртуальный
жесткий
диск
(или
диски
—
если
добавить
их
несколько)
размещены
они
будут
в
директории
/guest_images
Только
у
нас
эта
директория
будет
точкой
монтирования
отдельного
жесткого
диска
хост-сервера
специально
отведенного
для
этих
нужд
Безопасность
сохранения
данных
и
то
что
нужно
создавать
как
минимум
зеркальный
raid
массив
чтобы
не
потерять
данные
ВМ
в
случае
сбоя
жесткого
диска
мы
не
будем
так
как
это
—
отдельная
тема
Просмотрим
список
физических
дисков
на
хост-сервере:
#
fdisk
-l

Получился
вывод:
Disk
/dev/sda:
GB
bytes



Disk
/dev/sdb:
GB
bytes



На
жестком
диске
sda
установлена
ОС
его
не
трогаем
а
вот
на
sdb
создаем
раздел
на
все
свободное
место
диска
с
файловой
системой
ext
:
(более
подробно
про
следующие
операции
можно
почитать
)
Выбираем
диск
для
редактирования
#
fdisk
/dev/sdb

Создаем
новый
раздел
Command
(m
for
help):
n
Command
action

e
extended

p
primary
partition
(
-
)
p
Partition
number
(
-
):


Сохраняем
изменения
Command
(m
for
help):
w
The
partition
table
has
been
altered!

Создаем
файловую
систему
ext
на
всем
свободном
месте
диска
/dev/sdb
#
mkfs
ext
/dev/sdb


Создаем
точку
монтирования
нашего
жесткого
диска
для
файлов
виртуальных
машин:
#
mkdir
/guest_images

#
chmod
/guest_images
#
ls
-la
/guest_images

total

drwx------
root
root
May
:

dr-xr-xr-x
root
root
May
:


Многие
советуют
отключить
вообще
однако
мы
выберем
иной
путь
Мы
настроим
его
правильно
#
semanage
fcontext
-a
-t
virt_image_t
/guest_images

Если
выполнение
этой
команды
не
будет
успешным
надо
установить
дополнительный
пакет
Сначала
узнаем
какой
пакет
предоставляет
данную
команду
#
yum
provides
/usr/sbin/semanage

Получим
вывод:
Loaded
plugins:
rhnplugin

policycoreutils-python-
-
el
_
x
_
:
SELinux
policy
core
python
utilities

Repo
:
rhel-x
_
-server-

Matched
from:

Filename
:
/usr/sbin/semanage

policycoreutils-python-
-
el
x
_
:
SELinux
policy
core
python
utilities

Repo
:
rhel-x
_
-server-

Matched
from:

Filename
:
/usr/sbin/semanage


Устанавливаем
policycoreutils-python
#
yum
-y
install
policycoreutils-python

После
этого
снова:
#
semanage
fcontext
-a
-t
virt_image_t
/guest_images

Смонтируем
раздел
/dev/sdb
в
/guest_images
#
mount
-t
ext
/dev/sdb
/guest_images

Отредактируем
файл
/etc/fstab
для
того
чтобы
при
перезагрузке
хост-сервера
раздел
с
ВМ
монтировался
автоматически
#
vi
/etc/fstab

Добавляем
строку
по
примеру
тех
что
уже
имеются
в
файле
/dev/sdb
/guest_images
ext
defaults


Сохраняем
файл
и
продолжаем
создание
хранилища:
#
virsh
pool-define-as
guest_images_dir
dir
-
-
-
-
"/guest_images"

Pool
guest_images_dir
defined

Проверяем
создалось
ли
оно:
#
virsh
pool-list
--all

Name
State
Autostart

-----------------------------------------

default
active
yes

guest_images_dir
inactive
no

Далее:
#
virsh
pool-build
guest_images_dir

Pool
guest_images_dir
built


Запускаем
хранилище:
#
virsh
pool-start
guest_images_dir

Pool
guest_images_dir
started

#
virsh
pool-list
--all

Name
State
Autostart

-----------------------------------------

default
active
yes

guest_images_dir
active
no


Добавляем
в
автозагрузку:
#
virsh
pool-autostart
guest_images_dir

Pool
guest_images_dir
marked
as
autostarted

#
virsh
pool-list
--all

Name
State
Autostart

-----------------------------------------

default
active
yes

guest_images_dir
active
yes


Проверяем:
#
virsh
pool-info
guest_images_dir


Шаг
—
Настройка
сети
на
хост-сервере
!!!
ВАЖНО!!!
Перед
выполнением
этого
шага
надо
убедиться
что
на
хост-сервере
установлен
пакет
bridge-utils
#
rpm
-qa
|
grep
bridge-utils

иначе
при
выполнении
операций
с
сетью
вы
рискуете
потерять
связь
с
сервером
особенно
обидно
если
он
удаленный
и
физического
доступа
к
нему
у
вас
нет
Если
вывод
предыдущей
команды
пустой
то:
#
yum
-y
install
bridge-utils

Положим
что
для
выхода
«в
мир»
использовался
интерфейс
eth
и
он
был
соответствующим
образом
настроен
На
нем
настроен
IP-адрес
из
/
сети
маска
—
шлюз
Продолжаем
создаем
сетевой
интерфейс
типа
«bridge»
на
хост-сервере
#
vi
/etc/sysconfig/network-scripts/ifcfg-br


Содержимое
файла
DEVICE="br
"
NM_CONTROLLED="no"
ONBOOT="yes"
TYPE="Bridge"
BOOTPROTO="static"
IPADDR="
"
GATEWAY="
"
DNS
="
"
DNS
="
"
MTU="
"
NETMASK="
"
DEFROUTE="yes"
IPV
_FAILURE_FATAL="yes"
IPV
INIT="no"
NAME="System
br
"

Приводим
основной
сетевой
интерфейс
который
использовался
для
выхода
в
«мир»
к
виду:
#
vi
/etc/sysconfig/network-scripts/ifcfg-eth


DEVICE="eth
"
BOOTPROTO="none"
HOSTNAME="localhost
localdomain"


HWADDR="
:
C:
:
:
:
"


IPV
INIT="no"
MTU="
"
NM_CONTROLLED="no"
ONBOOT="yes"
TYPE="Ethernet"
NAME="System
eth
"
BRIDGE="br
"

!!!
Важно!!!
DEVICE=«eth
»
Имя
интерфейса
должно
остаться
таким
как
было
в
системе
Если
у
вас
для
выхода
в
Интернет
использовался
интерфейс
eth
тогда
редактировать
надо
его
HWADDR=«
:
C:C
:
:
:A
»
МАС-адрес
также
должен
остаться
таким
как
был
в
системе
Когда
проверили
все
перезагружаем
сеть:
#
service
network
restart

Проверяем
состояние
подключения
типа
«bridge»:
#
brctl
show

Получаем
что-то
вроде
этого
bridge
name
bridge
id
STP
enabled
interfaces
br
cc
a
no
eth


Делаем
настройки
в
iptables
чтобы
трафик
виртуалок
«ходил»
через
соединение
типа
bridge
#
iptables
-I
FORWARD
-m
physdev
--physdev-is-bridged
-j
ACCEPT
#
service
iptables
save
#
service
iptables
restart

Опционально:
можно
улучшить
быстродействие
соединения
bridge
поправив
настройки
в
/etc/sysctl
conf
net
bridge
bridge-nf-call-ip
tables
=

net
bridge
bridge-nf-call-iptables
=

net
bridge
bridge-nf-call-arptables
=


После
этого
#
sysctl
-p
/etc/sysctl
conf
#
service
libvirtd
reload

Шаг
—
Установка
новой
виртуальной
машины
Установка
CentOS
на
гостевую
ВМ:
virt-install
-n
VMName_
--ram
--arch=x
_
\
--vcpus=
--cpu
host
--check-cpu
\
--extra-args="vnc
sshd=
sshpw=secret
ip=static
reboot=b
selinux=
"
\
--os-type
linux
--os-variant=rhel
--boot
cdrom
hd
menu=on
\
--disk
pool=guest_images_dir
size=
bus=virtio
\
--network=bridge:br
model=virtio
\
--graphics
vnc
listen=
keymap=ru
password=some
password
here
\
--noautoconsole
--watchdog
default
action=reset
--virt-type=kvm
\
--autostart
--location
http://mirror
yandex
ru/centos/
/os/x
_
/

Примечание
:
VMName_
—
имя
новой
виртуальной
машины
–ram
—
кол-во
виртуальной
памяти
–arch=x
_
—
архитектура
ОС
виртуалки
–vcpus=
—
кол-во
виртуальных
процессоров
–os-type
linux
—
тип
ОС
–disk
pool=guest_images_dir
size=
—
размещение
хранилища
размер
вирт
диска
–network=bridge:br
Примечание
:
Если
на
ВМ
нужна
«белая
сеть»
тогда
ставим
--network=bridge:br
Если
на
ВМ
требуется
«серая
сеть»
тогда
ставим
--network=bridge:virbr
В
этом
случае
для
ВМ
будет
присвоен
серый
IP
по
DHCP
от
хост-сервера
--graphics
vnc
listen=
keymap=ru
password=some
password
here
Тут
указываем
пароль
для
подключения
к
ВМ
по
vnc
Установка
Windows
на
гостевую
ВМ:
virt-install
--connect
qemu:///system
--arch=x
_
\
-n
VMName_
-r
--vcpus=
\
--disk
pool=guest_images_dir
size=
bus=virtio
cache=none
\
-c
/iso/Windows
R
RU
ISO
--graphics
vnc
listen=
keymap=ru
password=some
password
here
\
--noautoconsole
--os-type
windows
--os-variant
win
k
\
--network=bridge:br
model=e
--disk
path=/iso/virtio-win
iso
device=cdrom
perms=ro

Примечание:
Параметры
такие
же
как
и
в
примере
с
установкой
CentOS
Но
есть
различия
При
установке
ОС
Windows
не
увидит
виртуального
жесткого
диска
поэтому
надо
подгрузить
дополнительный
виртуальный
cdrom
с
драйверами
/iso/virtio-win
iso
—
расположение
файла
ISO
с
драйверами
виртуального
диска
Взять
можно
Выполняем
команду
на
установку
новой
ВМ
затем
подключаемся
по
к
хост-серверу
для
продолжения
установки
ОС
Для
того
чтобы
узнать
порт
для
подключения
выполняем:
#
netstat
-nltp
|
grep
q

tcp
:
:*
LISTEN
/qemu-kvm

tcp
:
:*
LISTEN
/qemu-kvm
tcp
:
:*
LISTEN
/qemu-kvm

tcp
:
:*
LISTEN
/qemu-kvm

При
установке
новой
ВМ
порт
vnc-сервера
увеличится
на
При
удалении
ВМ
порт
освобождается
и
затем
выдается
новой
ВМ
То
есть
номер
порта
последней
ВМ
не
обязательно
самый
большой
из
…
Чтобы
узнать
на
каком
порту
vnc
виртуалка
с
определенным
названием
вводим:
#
virsh
vncdisplay
VMName_

:


где
VMName_
—
имя
ВМ
:
—
номер
по
порядку
порта
начиная
с
то
есть
подключаться
надо
на
порт
но
в
программе
UltraVNC
сработает
и
так
:
Примечание
Если
при
создании
ВМ
вылетает
ошибка
Permission
denied
kvm
не
может
открыть
файл
диска
ВМ
*
img
значит
надо
разрешить
выполнение
действий
qemu-kvm
из-под
root
(предполагается
что
управление
ВМ
производится
из-под
специально
созданного
для
этих
целей
пользователя
например
libvirt)
Но
мы
обойдемся
и
пользователем
root
Поправляем
конфиг:
#
vi
/etc/libvirt/qemu
conf

Находим
и
раскомментируем
в
нем
строки:
#
The
user
ID
for
QEMU
processes
run
by
the
system
instance

user
=
"root"

#
The
group
ID
for
QEMU
processes
run
by
the
system
instance

group
=
"root"


Полезно
знать:
Конфиги
ВМ
находятся
здесь
/etc/libvirt/qemu/
Для
того
чтобы
отредактировать
параметры
(добавить
процессор
ОЗУ
или
еще
что-то)
ищем
конфиг
ВМ
с
нужным
названием
редактируем:
#
vi
/etc/libvirt/qemu/VMName_
xml

К
примеру
можно
указать
статический
порт
vnc
для
конкретной
ВМ
чтобы
всегда
подключаться
к
нужному
порту
<graphics
type='vnc'
port='
'
autoport='no'
listen='
'
passwd='some
password
here'>
<listen
type='address'
address='
'/>
</graphics>

Теперь
у
этой
ВМ
порт
vnc
будет
—
Не
забудьте
перезагрузить
libvirtd
для
применения
изменений
Саму
ВМ
тоже
следует
перезагрузить
Поэтому
изменяйте
конфигурационный
файл
ВМ
пока
она
выключена
далее
выполняйте
service
libvirtd
reload
затем
стартуйте
ВМ
Команды
для
управления
ВМ:
virsh
-c
qemu:///system
help
Встроенная
помощь
по
командам



virsh
-c
qemu:///system
list
--all
Посмотреть
статус
установленных
ВМ



virsh
-c
qemu:///system
start
vsrv

Запусить
ВМ
vsrv



virsh
-c
qemu:///system
shutdown
vsrv

Послать
команду
завершения
работы
ВМ



virsh
-c
qemu:///system
destroy
vsrv

Принудительно
завершить
работу
ВМ



virsh
-c
qemu:///system
undefine
vsrv

Удалить
ВМ


Шаг
—
Настройка
сети
в
случае
«серых»
IP-адресов
в
ВМ
Если
на
шаге
вы
выбрали
серую
сеть
для
новой
ВМ
(--network=bridge:virbr
)
то
надо
выполнить
следующие
действия
(на
хост-сервере!)
для
проброса
трафика
на
ВМ
Разрешить
форвардинг
трафика
на
уровне
ядра
ОС:
#
sysctl
net
ipv
ip_forward=

#
iptables
-I
FORWARD
-j
ACCEPT
#
iptables
-t
nat
-I
PREROUTING
-p
tcp
-d
--dport
-j
DNAT
--to-destination
:


Здесь
—
белый
(внешний)
IP
хост-сервера
—
серый
IP-адрес
гостевой
ОС
#
iptables
-t
nat
-I
POSTROUTING
-p
tcp
-s
--sport
-j
SNAT
--to-source
:


На
примере
установки
ОС
CentOS
на
гостевой
машине
когда
установка
перешла
в
графический
режим
и
предлагает
подключиться
на
локальный
порт
гостевой
ОС
Подключаемся
из
ПК
за
которым
сидите
по
vnc
к
:
или
:
тоже
сработает
в
UltraVNC
По
такому
же
принципу
можно
прокинуть
порт
(стандартный)
RDP
или
SSH
в
гостевую
ОС
Шаг
—
Подготовка
к
управлению
виртуальными
машинами
удаленного
сервера
с
удобным
графическим
интерфейсом
(используя
virt-manager)
Есть
много
способов
«прокинуть»
графику
удаленного
сервера
на
ПК
за
которым
выполняете
действия
администрирования
Мы
остановимся
на
ssh-туннелировании
Положим
что
вы
выполняете
действия
с
локального
ПК
под
управлением
Windows
(в
операционных
системах
под
управлением
Linux
сделать
это
куда
легче
:)
нужно
выполнить
всего
одну
команду
ssh
-X
username@
конечно
с
оговоркой
что
на
удаленном
сервере
X
forwarding
разрешен
и
вы
сидите
за
локальным
Linux
ПК
c
графической
оболочкой)
тогда
нам
необходимо
Всем
знакомый
Порт
сервера
X
для
Windows
—
В
настройках
PuTTY
включить
«Enable
X
Forwarding»
Сделать
как
показано
на
картинке:
В
момент
подключения
к
удаленному
серверу
Xming
должен
быть
уже
запущен
На
хост-сервере
с
CentOS
для
SSH
включить
X
Forwarding
для
этого
отредактируйте
файл
sshd_config:
#
vi
/etc/ssh/sshd_config

X
Forwarding
yes
X
DisplayOffset

X
UseLocalhost
yes

После
этого
#
/etc/init
d/sshd
restart

Устанавливаем
virt-manager
на
хост-сервере:
#
yum
install
virt-manager

Еще
один
компонент
#
yum
-y
install
xorg-x
-xauth

Чтобы
окна
отображались
без
крякозябр
#
yum
install
liberation-sans-fonts

Шаг
—
Непосредственный
запуск
virt-manager
После
этого
надо
перезайти
по
SSH
к
удаленному
серверу
Xming
должен
быть
запущен
Запускаем
графическую
утилиту
управления
виртуальными
машинами
#
virt-manager

Откроется
окно
virt-manager
Консоль
управления
ВМ
Конфигурация
ВМ
и
ее
изменение
Надеюсь
читателю
понравилась
статья
Лично
я
прочитай
бы
подобную
в
своё
время
резко
сократил
бы
потраченное
время
на
то
чтобы
перелопатить
множество
мануалов
от
разных
админов
для
разных
ОС;
сохранил
бы
кучу
времени
потраченное
на
гугление
когда
появлялись
все
новые
и
новые
нюансы
Буду
рад
замечаниям
предложениям
по
данной
теме
Теги:
Хабы:
+
Карма
Рейтинг
Пользователь
Публикации
Лучшие
за
сутки
Похожие
Истории
Работа
вакансий
Ближайшие
события
–
февраля
:
Онлайн
–
февраля
Онлайн
февраля
:
Онлайн
–
февраля
Онлайн
февраля
:
Онлайн
февраля
:
Онлайн
февраля
:
Онлайн
февраля
:
Онлайн
февраля
:
Онлайн
февраля
:
Онлайн
марта
–
апреля
:
Онлайн
–
марта
:
–
:
Москва
•
Онлайн
марта
:
–
:
Москва
•
Онлайн
Ваш
аккаунт
Разделы
Информация
Услуги
Настройка
языка
©
–
